{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.446397066116 seconds ---\n",
      "--- time to normalize: 0.0425851345062 seconds ---\n",
      "--- time to segment: 0.487408876419 seconds ---\n",
      "--- time to extract features: 2.46152901649 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.763314\n",
      "And the confusion matrix is: \n",
      "[[314   5  57   1   1]\n",
      " [  1  46  20  12   5]\n",
      " [111   6 225   5   2]\n",
      " [  4  14   2  42  12]\n",
      " [  0  14   1   7 276]]\n",
      "In the 1 fold, the classification accuracy is 0.731192\n",
      "And the confusion matrix is: \n",
      "[[329   1  63   2   0]\n",
      " [  5  42  22  17   5]\n",
      " [110   6 189   4   2]\n",
      " [  5  18  17  23  22]\n",
      " [  0  10   1   8 282]]\n",
      "In the 2 fold, the classification accuracy is 0.731192\n",
      "And the confusion matrix is: \n",
      "[[299   3  61   0   0]\n",
      " [  0  35  19  25  13]\n",
      " [110   1 197   5   0]\n",
      " [  1  16  12  34  36]\n",
      " [  1   7   4   4 300]]\n",
      "In the 3 fold, the classification accuracy is 0.741336\n",
      "And the confusion matrix is: \n",
      "[[297   3  46   1   2]\n",
      " [  3  39  18  13   9]\n",
      " [106   4 215   7   1]\n",
      " [  3  19   6  42  31]\n",
      " [  1  18   3  12 284]]\n",
      "In the 4 fold, the classification accuracy is 0.706678\n",
      "And the confusion matrix is: \n",
      "[[280   5  61   1   4]\n",
      " [  2  29  26  13  14]\n",
      " [125   5 227   6   1]\n",
      " [  7   7  16  30  22]\n",
      " [  3  14   2  13 270]]\n",
      "In the 5 fold, the classification accuracy is 0.755706\n",
      "And the confusion matrix is: \n",
      "[[301   2  55   2   3]\n",
      " [  4  40  21  15  10]\n",
      " [ 94   3 220   3   2]\n",
      " [  7  19  10  41  22]\n",
      " [  0   7   2   8 292]]\n",
      "In the 6 fold, the classification accuracy is 0.740490\n",
      "And the confusion matrix is: \n",
      "[[272   4  61   4   2]\n",
      " [  1  36  23  12  10]\n",
      " [ 86   8 258   1   1]\n",
      " [  4  24  12  21  23]\n",
      " [  0  13   8  10 289]]\n",
      "In the 7 fold, the classification accuracy is 0.789518\n",
      "And the confusion matrix is: \n",
      "[[302   6  52   4   2]\n",
      " [  2  35  11  10  10]\n",
      " [ 86   9 246   2   2]\n",
      " [  2  11   7  60  13]\n",
      " [  0   8   6   6 291]]\n",
      "In the 8 fold, the classification accuracy is 0.751479\n",
      "And the confusion matrix is: \n",
      "[[303   4  43   0   3]\n",
      " [  4  29  18  15  21]\n",
      " [107   9 228   2   2]\n",
      " [  5  21  13  41  10]\n",
      " [  0  10   2   5 288]]\n",
      "In the 9 fold, the classification accuracy is 0.750634\n",
      "And the confusion matrix is: \n",
      "[[297   5  52   1   1]\n",
      " [  5  44  12  14  10]\n",
      " [114   6 219   5   0]\n",
      " [  4  15  13  29  19]\n",
      " [  2   6   2   9 299]]\n",
      "--- time to do Kfold: 24.4212839603 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to do Kfold: %s seconds ---\" % (evaluate_model - extracting_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.440854072571 seconds ---\n",
      "--- time to normalize: 0.0530569553375 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11830, 14, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "segs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830, 156)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
