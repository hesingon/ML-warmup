{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.370768070221 seconds ---\n",
      "--- time to normalize: 0.0338640213013 seconds ---\n",
      "--- time to segment: 0.433979988098 seconds ---\n",
      "--- time to extract features: 2.50458693504 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.751479\n",
      "And the confusion matrix is: \n",
      "[[301   1  49   2   3]\n",
      " [  4  45  15   9  17]\n",
      " [ 98   5 233   4   0]\n",
      " [  5  24   7  30  23]\n",
      " [  0  11   4  13 280]]\n",
      "In the 1 fold, the classification accuracy is 0.730347\n",
      "And the confusion matrix is: \n",
      "[[286   4  59   5   2]\n",
      " [  7  44  16  14   7]\n",
      " [121  10 210   2   2]\n",
      " [  5  26  10  43  13]\n",
      " [  1   5   4   6 281]]\n",
      "In the 2 fold, the classification accuracy is 0.752325\n",
      "And the confusion matrix is: \n",
      "[[284   1  47   3   2]\n",
      " [  4  34  14  22  10]\n",
      " [117   7 223   2   1]\n",
      " [  1  12   9  47  18]\n",
      " [  0   4   3  16 302]]\n",
      "In the 3 fold, the classification accuracy is 0.732037\n",
      "And the confusion matrix is: \n",
      "[[304   5  64   3   2]\n",
      " [  5  30  13  20  13]\n",
      " [102   2 195   4   1]\n",
      " [ 11  13  12  32  19]\n",
      " [  2   8   1  17 305]]\n",
      "In the 4 fold, the classification accuracy is 0.764159\n",
      "And the confusion matrix is: \n",
      "[[322   3  51   1   2]\n",
      " [  4  44  14  14   8]\n",
      " [104   6 215   3   0]\n",
      " [  2  21  10  32  17]\n",
      " [  0  11   4   4 291]]\n",
      "In the 5 fold, the classification accuracy is 0.730347\n",
      "And the confusion matrix is: \n",
      "[[288   1  61   4   2]\n",
      " [  5  40  17  15  14]\n",
      " [ 98   3 237   1   4]\n",
      " [  0  17  19  35  25]\n",
      " [  1  14   7  11 264]]\n",
      "In the 6 fold, the classification accuracy is 0.747253\n",
      "And the confusion matrix is: \n",
      "[[293   2  57   1   1]\n",
      " [  4  46  11  22   6]\n",
      " [129   6 195   4   1]\n",
      " [  2   8   9  46  13]\n",
      " [  4   6   3  10 304]]\n",
      "In the 7 fold, the classification accuracy is 0.744717\n",
      "And the confusion matrix is: \n",
      "[[309   2  48   1   3]\n",
      " [  3  29  24  11  13]\n",
      " [120   8 219   4   2]\n",
      " [  4   9   9  43  17]\n",
      " [  0  10   4  10 281]]\n",
      "In the 8 fold, the classification accuracy is 0.701606\n",
      "And the confusion matrix is: \n",
      "[[316   2  62   1   2]\n",
      " [  4  44  20  11   4]\n",
      " [134   8 173   1   5]\n",
      " [  1  22  19  25  35]\n",
      " [  2   8   4   8 272]]\n",
      "In the 9 fold, the classification accuracy is 0.757396\n",
      "And the confusion matrix is: \n",
      "[[309   1  43   2   0]\n",
      " [  1  33  14  14  13]\n",
      " [122   9 229   3   1]\n",
      " [  2  18  18  39  10]\n",
      " [  0   9   2   5 286]]\n",
      "--- time to extract features: 23.4814901352 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.87208136e-05,   7.65424768e-03,  -3.98987512e-03, ...,\n",
       "          2.08770027e-03,   1.45718677e-03,  -1.35025743e-03],\n",
       "       [ -3.38788457e-03,  -3.72409690e-03,  -3.17991818e-03, ...,\n",
       "         -2.96073633e-02,   9.56516229e-03,   5.84492524e-03],\n",
       "       [ -3.28204196e-03,  -2.38530300e-02,   3.86786618e-03, ...,\n",
       "         -1.64834330e-02,   2.71025473e-02,   7.01186619e-03],\n",
       "       ..., \n",
       "       [ -5.04473344e-02,   2.80414516e-02,  -7.28476400e-03, ...,\n",
       "          1.57633924e-01,   7.81363053e-02,   1.19943033e-01],\n",
       "       [  6.37063483e-02,  -3.55810374e-02,  -1.70506174e-02, ...,\n",
       "          1.71703230e-02,   6.26086851e-02,   2.82505322e-02],\n",
       "       [ -7.94067404e-02,   7.39349977e-03,  -2.37049611e-02, ...,\n",
       "         -4.67965236e-03,   2.24650548e-03,  -4.62998817e-04]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830, 156)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.372848987579 seconds ---\n",
      "--- time to normalize: 0.0270099639893 seconds ---\n",
      "--- time to segment: 0.41557097435 seconds ---\n",
      "--- time to extract features: 2.24505400658 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.723584\n",
      "And the confusion matrix is: \n",
      "[[314   6  52   2   0]\n",
      " [  2  35  25  15  11]\n",
      " [118   5 216   3   2]\n",
      " [  2  22  18  30  19]\n",
      " [  0  14   2   9 261]]\n",
      "In the 1 fold, the classification accuracy is 0.736264\n",
      "And the confusion matrix is: \n",
      "[[296   3  63   1   3]\n",
      " [  1  31  25  11   9]\n",
      " [107   5 217   5   0]\n",
      " [  1  18  12  31  18]\n",
      " [  2  10   5  13 296]]\n",
      "In the 2 fold, the classification accuracy is 0.757396\n",
      "And the confusion matrix is: \n",
      "[[281   3  66   4   0]\n",
      " [  5  44  10  18   6]\n",
      " [ 93  10 243   4   4]\n",
      " [  7  20   6  50  14]\n",
      " [  1  10   2   4 278]]\n",
      "In the 3 fold, the classification accuracy is 0.771767\n",
      "And the confusion matrix is: \n",
      "[[318   2  49   1   4]\n",
      " [  0  44  17  20   5]\n",
      " [105   7 222   1   2]\n",
      " [  1  19   9  42  13]\n",
      " [  3   5   1   6 287]]\n",
      "In the 4 fold, the classification accuracy is 0.732037\n",
      "And the confusion matrix is: \n",
      "[[275   3  53   4   1]\n",
      " [  4  38  15  20   6]\n",
      " [109  10 246   1   1]\n",
      " [  3  22  16  27  22]\n",
      " [  0   6   3  18 280]]\n",
      "In the 5 fold, the classification accuracy is 0.725275\n",
      "And the confusion matrix is: \n",
      "[[301   2  39   3   1]\n",
      " [  3  35  17  27   9]\n",
      " [124   3 190   2   3]\n",
      " [  2  29  15  24  20]\n",
      " [  0  10   3  13 308]]\n",
      "In the 6 fold, the classification accuracy is 0.731192\n",
      "And the confusion matrix is: \n",
      "[[318   3  66   1   2]\n",
      " [  5  42  20  12  11]\n",
      " [100   6 213   2   2]\n",
      " [  7  20  12  18  25]\n",
      " [  2   9   6   7 274]]\n",
      "In the 7 fold, the classification accuracy is 0.755706\n",
      "And the confusion matrix is: \n",
      "[[282   3  61   3   5]\n",
      " [  9  21  21  16  16]\n",
      " [ 76   5 247   5   1]\n",
      " [ 10   5  14  42  19]\n",
      " [  1   6   3  10 302]]\n",
      "In the 8 fold, the classification accuracy is 0.742181\n",
      "And the confusion matrix is: \n",
      "[[282   1  58   2   2]\n",
      " [  5  40  16  13   7]\n",
      " [103   4 228   4   2]\n",
      " [  6  17  13  39  18]\n",
      " [  1  14   3  16 289]]\n",
      "In the 9 fold, the classification accuracy is 0.749789\n",
      "And the confusion matrix is: \n",
      "[[326   3  46   0   3]\n",
      " [  1  37  12  20  13]\n",
      " [113   5 206   2   1]\n",
      " [  2  21  12  39  16]\n",
      " [  1  13   2  10 279]]\n",
      "--- time to extract features: 21.2362270355 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.384927988052 seconds ---\n",
      "--- time to normalize: 0.027195930481 seconds ---\n",
      "--- time to segment: 0.471884012222 seconds ---\n",
      "--- time to extract features: 2.31225395203 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.715131\n",
      "And the confusion matrix is: \n",
      "[[331   5  41   1   1]\n",
      " [  0  40   8  13   4]\n",
      " [134  13 180   4   3]\n",
      " [  1  32  11  18  38]\n",
      " [  1  14   3  10 277]]\n",
      "In the 1 fold, the classification accuracy is 0.710059\n",
      "And the confusion matrix is: \n",
      "[[291   8  60   0   4]\n",
      " [  2  43  16  23  10]\n",
      " [107   8 206   4   5]\n",
      " [  0  29   9  21  35]\n",
      " [  0   7   0  16 279]]\n",
      "In the 2 fold, the classification accuracy is 0.746407\n",
      "And the confusion matrix is: \n",
      "[[305   6  53   1   3]\n",
      " [  0  32  11  31   6]\n",
      " [ 96   9 193   4   5]\n",
      " [  0  24   8  39  23]\n",
      " [  0   3   0  17 314]]\n",
      "In the 3 fold, the classification accuracy is 0.736264\n",
      "And the confusion matrix is: \n",
      "[[313   7  50   3   3]\n",
      " [  1  54   7  30   5]\n",
      " [108  21 200   4   1]\n",
      " [  0  21   7  28  21]\n",
      " [  0   6   0  17 276]]\n",
      "In the 4 fold, the classification accuracy is 0.718512\n",
      "And the confusion matrix is: \n",
      "[[314   4  40   1   2]\n",
      " [  0  48  11  12  12]\n",
      " [137  19 177   0   3]\n",
      " [  2  23  10  14  42]\n",
      " [  0   7   0   8 297]]\n",
      "In the 5 fold, the classification accuracy is 0.711750\n",
      "And the confusion matrix is: \n",
      "[[291   4  60   3   5]\n",
      " [  0  33  16  26   7]\n",
      " [104  19 221   8   0]\n",
      " [  1  32   5  26  27]\n",
      " [  0   3   0  21 271]]\n",
      "In the 6 fold, the classification accuracy is 0.701606\n",
      "And the confusion matrix is: \n",
      "[[290   5  54   0   1]\n",
      " [  1  41  17  32   6]\n",
      " [120  10 193   3   2]\n",
      " [  0  33   9  21  30]\n",
      " [  1   5   1  23 285]]\n",
      "In the 7 fold, the classification accuracy is 0.732037\n",
      "And the confusion matrix is: \n",
      "[[286  11  52   5   1]\n",
      " [  1  41  11  20   5]\n",
      " [117  16 205   3   0]\n",
      " [  1  24   3  30  15]\n",
      " [  0   7   1  24 304]]\n",
      "In the 8 fold, the classification accuracy is 0.721893\n",
      "And the confusion matrix is: \n",
      "[[287   6  53   1   2]\n",
      " [  2  51  15  18  10]\n",
      " [103  21 217   5   3]\n",
      " [  1  27   9  18  27]\n",
      " [  0   3   0  23 281]]\n",
      "In the 9 fold, the classification accuracy is 0.714286\n",
      "And the confusion matrix is: \n",
      "[[295   0  54   2   2]\n",
      " [  0  32  20  19   2]\n",
      " [122  15 226   6   3]\n",
      " [  2  21  13  28  28]\n",
      " [  1   3   3  22 264]]\n",
      "--- time to extract features: 23.2225720882 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15, 10), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.406511068344 seconds ---\n",
      "--- time to normalize: 0.0289750099182 seconds ---\n",
      "--- time to segment: 0.430980920792 seconds ---\n",
      "--- time to extract features: 2.33887004852 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.783601\n",
      "And the confusion matrix is: \n",
      "[[295   3  72   0   2]\n",
      " [  0  44  14  14   0]\n",
      " [ 92   7 239   3   0]\n",
      " [  0  16   5  50  12]\n",
      " [  0   4   3   9 299]]\n",
      "In the 1 fold, the classification accuracy is 0.770921\n",
      "And the confusion matrix is: \n",
      "[[297   4  59   1   1]\n",
      " [  0  39  14  17  11]\n",
      " [ 72   8 241   4   0]\n",
      " [  1  26  10  46  24]\n",
      " [  0   7   2  10 289]]\n",
      "In the 2 fold, the classification accuracy is 0.777684\n",
      "And the confusion matrix is: \n",
      "[[289   4  69   1   2]\n",
      " [  0  50  18  19   6]\n",
      " [ 86   3 241   2   1]\n",
      " [  0  12  10  43  20]\n",
      " [  0   1   2   7 297]]\n",
      "In the 3 fold, the classification accuracy is 0.777684\n",
      "And the confusion matrix is: \n",
      "[[282   6  49   1   2]\n",
      " [  0  42  18  18   8]\n",
      " [ 92   8 238   4   1]\n",
      " [  2  14  10  54  15]\n",
      " [  2   5   0   8 304]]\n",
      "In the 4 fold, the classification accuracy is 0.756551\n",
      "And the confusion matrix is: \n",
      "[[274   9  66   2   1]\n",
      " [  0  58  14  21   5]\n",
      " [100   8 238   3   2]\n",
      " [  0   9   3  52  24]\n",
      " [  0  12   1   8 273]]\n",
      "In the 5 fold, the classification accuracy is 0.778529\n",
      "And the confusion matrix is: \n",
      "[[291   6  59   1   1]\n",
      " [  0  53  21  12   6]\n",
      " [ 93   4 235   4   2]\n",
      " [  0  12  10  42  17]\n",
      " [  0   7   3   4 300]]\n",
      "In the 6 fold, the classification accuracy is 0.737954\n",
      "And the confusion matrix is: \n",
      "[[286   2  55   2   4]\n",
      " [  0  31  25  29   5]\n",
      " [120   6 201   4   1]\n",
      " [  1   6   9  47  21]\n",
      " [  0   4   1  15 308]]\n",
      "In the 7 fold, the classification accuracy is 0.783601\n",
      "And the confusion matrix is: \n",
      "[[304   4  71   1   2]\n",
      " [  1  46   9  13   8]\n",
      " [ 81   4 237   4   0]\n",
      " [  2  15   7  45  15]\n",
      " [  0   8   4   7 295]]\n",
      "In the 8 fold, the classification accuracy is 0.788673\n",
      "And the confusion matrix is: \n",
      "[[305   4  68   2   3]\n",
      " [  0  43  12  18   5]\n",
      " [ 85   6 256   1   2]\n",
      " [  0   9   9  52  12]\n",
      " [  0   8   0   6 277]]\n",
      "In the 9 fold, the classification accuracy is 0.732883\n",
      "And the confusion matrix is: \n",
      "[[289   0  62   2   2]\n",
      " [  0  44  15  16   3]\n",
      " [145   5 190   4   0]\n",
      " [  0  16   7  55  20]\n",
      " [  1   8   2   8 289]]\n",
      "--- time to extract features: 60.1783668995 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(60, 30), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MLPClassifier.get_params of MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(60, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830, 156)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to train model: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
