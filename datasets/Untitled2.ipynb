{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./dataset-har-PUC-Rio-ugulino.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-699d91c2ab05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m### Retrieving all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moverall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dataset-har-PUC-Rio-ugulino.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"z4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# has to be converted to ndarray in order to be processed by segment_signal()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"class,,\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# double commas: looks like the researchers are naughty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Darius/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Darius/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Darius/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Darius/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Darius/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./dataset-har-PUC-Rio-ugulino.csv does not exist"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.87208136e-05,   7.65424768e-03,  -3.98987512e-03, ...,\n",
       "          2.08770027e-03,   1.45718677e-03,  -1.35025743e-03],\n",
       "       [ -3.38788457e-03,  -3.72409690e-03,  -3.17991818e-03, ...,\n",
       "         -2.96073633e-02,   9.56516229e-03,   5.84492524e-03],\n",
       "       [ -3.28204196e-03,  -2.38530300e-02,   3.86786618e-03, ...,\n",
       "         -1.64834330e-02,   2.71025473e-02,   7.01186619e-03],\n",
       "       ..., \n",
       "       [ -5.04473344e-02,   2.80414516e-02,  -7.28476400e-03, ...,\n",
       "          1.57633924e-01,   7.81363053e-02,   1.19943033e-01],\n",
       "       [  6.37063483e-02,  -3.55810374e-02,  -1.70506174e-02, ...,\n",
       "          1.71703230e-02,   6.26086851e-02,   2.82505322e-02],\n",
       "       [ -7.94067404e-02,   7.39349977e-03,  -2.37049611e-02, ...,\n",
       "         -4.67965236e-03,   2.24650548e-03,  -4.62998817e-04]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830, 156)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.372848987579 seconds ---\n",
      "--- time to normalize: 0.0270099639893 seconds ---\n",
      "--- time to segment: 0.41557097435 seconds ---\n",
      "--- time to extract features: 2.24505400658 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.723584\n",
      "And the confusion matrix is: \n",
      "[[314   6  52   2   0]\n",
      " [  2  35  25  15  11]\n",
      " [118   5 216   3   2]\n",
      " [  2  22  18  30  19]\n",
      " [  0  14   2   9 261]]\n",
      "In the 1 fold, the classification accuracy is 0.736264\n",
      "And the confusion matrix is: \n",
      "[[296   3  63   1   3]\n",
      " [  1  31  25  11   9]\n",
      " [107   5 217   5   0]\n",
      " [  1  18  12  31  18]\n",
      " [  2  10   5  13 296]]\n",
      "In the 2 fold, the classification accuracy is 0.757396\n",
      "And the confusion matrix is: \n",
      "[[281   3  66   4   0]\n",
      " [  5  44  10  18   6]\n",
      " [ 93  10 243   4   4]\n",
      " [  7  20   6  50  14]\n",
      " [  1  10   2   4 278]]\n",
      "In the 3 fold, the classification accuracy is 0.771767\n",
      "And the confusion matrix is: \n",
      "[[318   2  49   1   4]\n",
      " [  0  44  17  20   5]\n",
      " [105   7 222   1   2]\n",
      " [  1  19   9  42  13]\n",
      " [  3   5   1   6 287]]\n",
      "In the 4 fold, the classification accuracy is 0.732037\n",
      "And the confusion matrix is: \n",
      "[[275   3  53   4   1]\n",
      " [  4  38  15  20   6]\n",
      " [109  10 246   1   1]\n",
      " [  3  22  16  27  22]\n",
      " [  0   6   3  18 280]]\n",
      "In the 5 fold, the classification accuracy is 0.725275\n",
      "And the confusion matrix is: \n",
      "[[301   2  39   3   1]\n",
      " [  3  35  17  27   9]\n",
      " [124   3 190   2   3]\n",
      " [  2  29  15  24  20]\n",
      " [  0  10   3  13 308]]\n",
      "In the 6 fold, the classification accuracy is 0.731192\n",
      "And the confusion matrix is: \n",
      "[[318   3  66   1   2]\n",
      " [  5  42  20  12  11]\n",
      " [100   6 213   2   2]\n",
      " [  7  20  12  18  25]\n",
      " [  2   9   6   7 274]]\n",
      "In the 7 fold, the classification accuracy is 0.755706\n",
      "And the confusion matrix is: \n",
      "[[282   3  61   3   5]\n",
      " [  9  21  21  16  16]\n",
      " [ 76   5 247   5   1]\n",
      " [ 10   5  14  42  19]\n",
      " [  1   6   3  10 302]]\n",
      "In the 8 fold, the classification accuracy is 0.742181\n",
      "And the confusion matrix is: \n",
      "[[282   1  58   2   2]\n",
      " [  5  40  16  13   7]\n",
      " [103   4 228   4   2]\n",
      " [  6  17  13  39  18]\n",
      " [  1  14   3  16 289]]\n",
      "In the 9 fold, the classification accuracy is 0.749789\n",
      "And the confusion matrix is: \n",
      "[[326   3  46   0   3]\n",
      " [  1  37  12  20  13]\n",
      " [113   5 206   2   1]\n",
      " [  2  21  12  39  16]\n",
      " [  1  13   2  10 279]]\n",
      "--- time to extract features: 21.2362270355 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.384927988052 seconds ---\n",
      "--- time to normalize: 0.027195930481 seconds ---\n",
      "--- time to segment: 0.471884012222 seconds ---\n",
      "--- time to extract features: 2.31225395203 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.715131\n",
      "And the confusion matrix is: \n",
      "[[331   5  41   1   1]\n",
      " [  0  40   8  13   4]\n",
      " [134  13 180   4   3]\n",
      " [  1  32  11  18  38]\n",
      " [  1  14   3  10 277]]\n",
      "In the 1 fold, the classification accuracy is 0.710059\n",
      "And the confusion matrix is: \n",
      "[[291   8  60   0   4]\n",
      " [  2  43  16  23  10]\n",
      " [107   8 206   4   5]\n",
      " [  0  29   9  21  35]\n",
      " [  0   7   0  16 279]]\n",
      "In the 2 fold, the classification accuracy is 0.746407\n",
      "And the confusion matrix is: \n",
      "[[305   6  53   1   3]\n",
      " [  0  32  11  31   6]\n",
      " [ 96   9 193   4   5]\n",
      " [  0  24   8  39  23]\n",
      " [  0   3   0  17 314]]\n",
      "In the 3 fold, the classification accuracy is 0.736264\n",
      "And the confusion matrix is: \n",
      "[[313   7  50   3   3]\n",
      " [  1  54   7  30   5]\n",
      " [108  21 200   4   1]\n",
      " [  0  21   7  28  21]\n",
      " [  0   6   0  17 276]]\n",
      "In the 4 fold, the classification accuracy is 0.718512\n",
      "And the confusion matrix is: \n",
      "[[314   4  40   1   2]\n",
      " [  0  48  11  12  12]\n",
      " [137  19 177   0   3]\n",
      " [  2  23  10  14  42]\n",
      " [  0   7   0   8 297]]\n",
      "In the 5 fold, the classification accuracy is 0.711750\n",
      "And the confusion matrix is: \n",
      "[[291   4  60   3   5]\n",
      " [  0  33  16  26   7]\n",
      " [104  19 221   8   0]\n",
      " [  1  32   5  26  27]\n",
      " [  0   3   0  21 271]]\n",
      "In the 6 fold, the classification accuracy is 0.701606\n",
      "And the confusion matrix is: \n",
      "[[290   5  54   0   1]\n",
      " [  1  41  17  32   6]\n",
      " [120  10 193   3   2]\n",
      " [  0  33   9  21  30]\n",
      " [  1   5   1  23 285]]\n",
      "In the 7 fold, the classification accuracy is 0.732037\n",
      "And the confusion matrix is: \n",
      "[[286  11  52   5   1]\n",
      " [  1  41  11  20   5]\n",
      " [117  16 205   3   0]\n",
      " [  1  24   3  30  15]\n",
      " [  0   7   1  24 304]]\n",
      "In the 8 fold, the classification accuracy is 0.721893\n",
      "And the confusion matrix is: \n",
      "[[287   6  53   1   2]\n",
      " [  2  51  15  18  10]\n",
      " [103  21 217   5   3]\n",
      " [  1  27   9  18  27]\n",
      " [  0   3   0  23 281]]\n",
      "In the 9 fold, the classification accuracy is 0.714286\n",
      "And the confusion matrix is: \n",
      "[[295   0  54   2   2]\n",
      " [  0  32  20  19   2]\n",
      " [122  15 226   6   3]\n",
      " [  2  21  13  28  28]\n",
      " [  1   3   3  22 264]]\n",
      "--- time to extract features: 23.2225720882 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15, 10), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.406511068344 seconds ---\n",
      "--- time to normalize: 0.0289750099182 seconds ---\n",
      "--- time to segment: 0.430980920792 seconds ---\n",
      "--- time to extract features: 2.33887004852 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.783601\n",
      "And the confusion matrix is: \n",
      "[[295   3  72   0   2]\n",
      " [  0  44  14  14   0]\n",
      " [ 92   7 239   3   0]\n",
      " [  0  16   5  50  12]\n",
      " [  0   4   3   9 299]]\n",
      "In the 1 fold, the classification accuracy is 0.770921\n",
      "And the confusion matrix is: \n",
      "[[297   4  59   1   1]\n",
      " [  0  39  14  17  11]\n",
      " [ 72   8 241   4   0]\n",
      " [  1  26  10  46  24]\n",
      " [  0   7   2  10 289]]\n",
      "In the 2 fold, the classification accuracy is 0.777684\n",
      "And the confusion matrix is: \n",
      "[[289   4  69   1   2]\n",
      " [  0  50  18  19   6]\n",
      " [ 86   3 241   2   1]\n",
      " [  0  12  10  43  20]\n",
      " [  0   1   2   7 297]]\n",
      "In the 3 fold, the classification accuracy is 0.777684\n",
      "And the confusion matrix is: \n",
      "[[282   6  49   1   2]\n",
      " [  0  42  18  18   8]\n",
      " [ 92   8 238   4   1]\n",
      " [  2  14  10  54  15]\n",
      " [  2   5   0   8 304]]\n",
      "In the 4 fold, the classification accuracy is 0.756551\n",
      "And the confusion matrix is: \n",
      "[[274   9  66   2   1]\n",
      " [  0  58  14  21   5]\n",
      " [100   8 238   3   2]\n",
      " [  0   9   3  52  24]\n",
      " [  0  12   1   8 273]]\n",
      "In the 5 fold, the classification accuracy is 0.778529\n",
      "And the confusion matrix is: \n",
      "[[291   6  59   1   1]\n",
      " [  0  53  21  12   6]\n",
      " [ 93   4 235   4   2]\n",
      " [  0  12  10  42  17]\n",
      " [  0   7   3   4 300]]\n",
      "In the 6 fold, the classification accuracy is 0.737954\n",
      "And the confusion matrix is: \n",
      "[[286   2  55   2   4]\n",
      " [  0  31  25  29   5]\n",
      " [120   6 201   4   1]\n",
      " [  1   6   9  47  21]\n",
      " [  0   4   1  15 308]]\n",
      "In the 7 fold, the classification accuracy is 0.783601\n",
      "And the confusion matrix is: \n",
      "[[304   4  71   1   2]\n",
      " [  1  46   9  13   8]\n",
      " [ 81   4 237   4   0]\n",
      " [  2  15   7  45  15]\n",
      " [  0   8   4   7 295]]\n",
      "In the 8 fold, the classification accuracy is 0.788673\n",
      "And the confusion matrix is: \n",
      "[[305   4  68   2   3]\n",
      " [  0  43  12  18   5]\n",
      " [ 85   6 256   1   2]\n",
      " [  0   9   9  52  12]\n",
      " [  0   8   0   6 277]]\n",
      "In the 9 fold, the classification accuracy is 0.732883\n",
      "And the confusion matrix is: \n",
      "[[289   0  62   2   2]\n",
      " [  0  44  15  16   3]\n",
      " [145   5 190   4   0]\n",
      " [  0  16   7  55  20]\n",
      " [  1   8   2   8 289]]\n",
      "--- time to extract features: 60.1783668995 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(60, 30), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MLPClassifier.get_params of MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(60, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830, 156)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(15,), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to train model: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.39012503624 seconds ---\n",
      "--- time to normalize: 0.0280389785767 seconds ---\n",
      "--- time to segment: 0.456530094147 seconds ---\n",
      "--- time to extract features: 2.34912204742 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.732883\n",
      "And the confusion matrix is: \n",
      "[[296   9  57   0   2]\n",
      " [  0  52   3  16   8]\n",
      " [108  17 197   4   3]\n",
      " [  0  38   8  26  22]\n",
      " [  0   8   2  11 296]]\n",
      "In the 1 fold, the classification accuracy is 0.725275\n",
      "And the confusion matrix is: \n",
      "[[311   5  53   3   3]\n",
      " [  0  39  12  24   5]\n",
      " [111  22 203   1   2]\n",
      " [  0  29  10  27  27]\n",
      " [  1   5   1  11 278]]\n",
      "In the 2 fold, the classification accuracy is 0.735418\n",
      "And the confusion matrix is: \n",
      "[[303   4  51   4   0]\n",
      " [  2  27  21  23   6]\n",
      " [ 92  14 225   7   3]\n",
      " [  0  19  13  32  26]\n",
      " [  0   4   2  22 283]]\n",
      "In the 3 fold, the classification accuracy is 0.727811\n",
      "And the confusion matrix is: \n",
      "[[311   3  42   2   1]\n",
      " [  0  42   9  27   3]\n",
      " [119  21 192   7   2]\n",
      " [  0  28  10  37  18]\n",
      " [  0   4   1  25 279]]\n",
      "In the 4 fold, the classification accuracy is 0.699915\n",
      "And the confusion matrix is: \n",
      "[[318   6  50   3   2]\n",
      " [  1  35  23  27   5]\n",
      " [133  13 188   4   0]\n",
      " [  1  16  16  26  32]\n",
      " [  0   3   1  19 261]]\n",
      "In the 5 fold, the classification accuracy is 0.710059\n",
      "And the confusion matrix is: \n",
      "[[299  12  43   1   4]\n",
      " [  2  47  11  18   6]\n",
      " [135  30 189   1   3]\n",
      " [  2  27   6  21  21]\n",
      " [  0   8   0  13 284]]\n",
      "In the 6 fold, the classification accuracy is 0.722739\n",
      "And the confusion matrix is: \n",
      "[[304   9  42   0   1]\n",
      " [  2  54  12   7  14]\n",
      " [114  24 180   3   1]\n",
      " [  0  23   6  14  43]\n",
      " [  2  11   1  13 303]]\n",
      "In the 7 fold, the classification accuracy is 0.755706\n",
      "And the confusion matrix is: \n",
      "[[294   0  69   3   1]\n",
      " [  0  58  29   6   6]\n",
      " [ 91   6 216   0   2]\n",
      " [  1  19  11  30  17]\n",
      " [  0   8   0  20 296]]\n",
      "In the 8 fold, the classification accuracy is 0.728656\n",
      "And the confusion matrix is: \n",
      "[[290   4  55   1   5]\n",
      " [  0  45  10  20   8]\n",
      " [117  19 225   3   2]\n",
      " [  2  25  10  25  30]\n",
      " [  0   0   2   8 277]]\n",
      "In the 9 fold, the classification accuracy is 0.722739\n",
      "And the confusion matrix is: \n",
      "[[289   5  44   0   3]\n",
      " [  0  48  14  15   3]\n",
      " [130  14 183   4   3]\n",
      " [  0  33  10  32  18]\n",
      " [  0   8   0  24 303]]\n",
      "--- time for kfold: 25.5934250355 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "layer_1_val = 15\n",
    "layer_2_val = 10\n",
    "\n",
    "#having 15 neurons\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(layer_1_val, layer_2_val), random_state=1).fit(features[train], y[train])\n",
    "    predictions = clf.predict(features[test])\n",
    "    accuracy = clf.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time for kfold: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- time to load and select datasets: 0.424869060516 seconds ---\n",
      "--- time to normalize: 0.0486030578613 seconds ---\n",
      "--- time to segment: 0.479005098343 seconds ---\n",
      "--- time to extract features: 2.27210497856 seconds ---\n",
      "In the 0 fold, the classification accuracy is 0.371090 and the recall is 0.256233\n",
      "And the confusion matrix is: \n",
      "[[360   0   2   0   0]\n",
      " [ 47   4  16   0   9]\n",
      " [313   0  23   0   0]\n",
      " [ 53   7  23   0  12]\n",
      " [137  21  95   9  52]]\n",
      "In the 1 fold, the classification accuracy is 0.359256 and the recall is 0.254490\n",
      "And the confusion matrix is: \n",
      "[[334   0  16   0   1]\n",
      " [ 53   4  30   0  15]\n",
      " [301   0  29   0   2]\n",
      " [ 47   2  29   1   7]\n",
      " [131  20  99   5  57]]\n",
      "In the 2 fold, the classification accuracy is 0.326289 and the recall is 0.240185\n",
      "And the confusion matrix is: \n",
      "[[323   0   3   2   1]\n",
      " [ 62   3  25   1   7]\n",
      " [351   1  11   1   3]\n",
      " [ 46   2  22   0  10]\n",
      " [148  23  80   9  49]]\n",
      "In the 3 fold, the classification accuracy is 0.347422 and the recall is 0.258999\n",
      "And the confusion matrix is: \n",
      "[[337   0   4   0   1]\n",
      " [ 46   6  19   0   8]\n",
      " [337   0  23   0   3]\n",
      " [ 54   6  28   3   8]\n",
      " [154  18  82   4  42]]\n",
      "In the 4 fold, the classification accuracy is 0.349112 and the recall is 0.245398\n",
      "And the confusion matrix is: \n",
      "[[340   0  11   0   2]\n",
      " [ 48   2  17   2  10]\n",
      " [313   0  16   0   1]\n",
      " [ 40   5  24   3  12]\n",
      " [149  24  99  13  52]]\n",
      "In the 5 fold, the classification accuracy is 0.366019 and the recall is 0.253234\n",
      "And the confusion matrix is: \n",
      "[[349   0  19   0   0]\n",
      " [ 51   6  36   0  10]\n",
      " [277   0  24   0   1]\n",
      " [ 57   6  22   1  12]\n",
      " [126  14 113   6  53]]\n",
      "In the 6 fold, the classification accuracy is 0.383770 and the recall is 0.261046\n",
      "And the confusion matrix is: \n",
      "[[375   0   6   1   0]\n",
      " [ 41   4  17   2   9]\n",
      " [326   0  20   0   4]\n",
      " [ 36   5  21   3  10]\n",
      " [115  26 104   6  52]]\n",
      "In the 7 fold, the classification accuracy is 0.393914 and the recall is 0.268618\n",
      "And the confusion matrix is: \n",
      "[[364   0  28   0   2]\n",
      " [ 46   4  23   0   7]\n",
      " [268   0  38   0   0]\n",
      " [ 38   6  34   7  14]\n",
      " [125  26  87  13  53]]\n",
      "In the 8 fold, the classification accuracy is 0.360947 and the recall is 0.252486\n",
      "And the confusion matrix is: \n",
      "[[353   0  12   0   0]\n",
      " [ 35   4  19   0  15]\n",
      " [312   0  27   0   2]\n",
      " [ 42   4  35   3  14]\n",
      " [144  18  83  21  40]]\n",
      "In the 9 fold, the classification accuracy is 0.368555 and the recall is 0.249348\n",
      "And the confusion matrix is: \n",
      "[[362   1   7   0   1]\n",
      " [ 51   2  19   0  10]\n",
      " [330   0  26   0   0]\n",
      " [ 28   1  31   2  11]\n",
      " [129  25  98   5  44]]\n",
      "--- time to extract features: 797.867202997 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# description of this dataset http://groupware.les.inf.puc-rio.br/har#ixzz2PyRdbAfA\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "le = pp.LabelEncoder() \n",
    "le.fit(['sitting', 'walking', 'sittingdown', 'standing', 'standingup'])\n",
    "\n",
    "initial = time.time()\n",
    "### Retrieving all data\n",
    "overall = pd.read_csv(\"./dataset-har-PUC-Rio-ugulino.csv\", delimiter=';', header='infer') \n",
    "data = overall.loc[:, \"x1\":\"z4\"].as_matrix() # has to be converted to ndarray in order to be processed by segment_signal()\n",
    "targets = overall.loc[:,\"class,,\"].as_matrix() # double commas: looks like the researchers are naughty\n",
    "\n",
    "load = time.time()\n",
    "print \"--- time to load and select datasets: %s seconds ---\" % (load - initial)\n",
    "\n",
    "\n",
    "### Data segmentation: shall use a sudden change of sensor readings\n",
    "### like if (x_pre - x_curr <= 1.0, do nothing)\n",
    "### Range of Accelerometer sensor readings is +3g/-3g\n",
    "\n",
    "# reading 14 sets of data in every 2 seconds. \n",
    "# For segmenting the data from online only. \n",
    "# each set of data is taken 150ms apart from another.\n",
    "# so choosing a window size of 14 will be 2.1 seconds.\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=14): \n",
    "\n",
    "    N = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    K = N/window_size\n",
    "    segments = numpy.empty((K, window_size, dim))\n",
    "    for i in range(K):\n",
    "        segment = data[i*window_size:i*window_size+window_size,:]\n",
    "        segments[i] = numpy.vstack(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "##!!!! questions: for normalization, should it be done right after loading csv or after segmenation? \n",
    "##!!!! Normalize() can't process nadarray with dimension > 2.\n",
    "X = pp.normalize(data)\n",
    "y = targets[::14] \n",
    "y = y[:-1]# -1 because it will have a extra set of data than X.\n",
    "\n",
    "normalizing = time.time()\n",
    "print \"--- time to normalize: %s seconds ---\" % (normalizing - load)\n",
    "\n",
    "segs = segment_signal(X)\n",
    "\n",
    "segmenting = time.time()\n",
    "print \"--- time to segment: %s seconds ---\" % (segmenting - normalizing)\n",
    "\n",
    "### feautre extraction // take the difference between sensors\n",
    "\n",
    "### this method is to extract the difference between consecutive sensor readings.\n",
    "## parameter raw is a 2D ndarray\n",
    "## return a 2D ndarray\n",
    "def extract_diff(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of sets of sensor readings\n",
    "    dim = raw.shape[1] # number of values in each readings\n",
    "    features = numpy.empty((N - 1, dim))\n",
    "    for i in range(1, N):\n",
    "        for j in range(dim):\n",
    "            features[i-1][j] = raw[i][j] - raw[i-1][j]\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_diff_2(raw):\n",
    "\n",
    "    N = raw.shape[0] # number of segments of sensor readings ()\n",
    "    I = raw.shape[1] # number of sets of readings (14)\n",
    "    J = raw.shape[2] # number of values in each set of readings (12)\n",
    "    feature_num = (I - 1) * J\n",
    "    feature = numpy.empty((feature_num))\n",
    "    features = numpy.empty((N, feature_num))\n",
    "    for n in range(N):\n",
    "        idx = 0;\n",
    "        for i in range(1, I):\n",
    "            for j in range(J):\n",
    "                feature[idx] = raw[n][i][j] - raw[n][i-1][j]\n",
    "                idx += 1\n",
    "        features[n] = feature\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "features = extract_diff_2(segs)\n",
    "\n",
    "extracting_feature = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (extracting_feature - segmenting)\n",
    "\n",
    "layer_1_val = 15\n",
    "layer_2_val = 10\n",
    "\n",
    "################################################################\n",
    "#having 15 neurons\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# fold_index = 0\n",
    "# for train, test in kfold.split(features):\n",
    "#     clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                      hidden_layer_sizes=(layer_1_val, layer_2_val), random_state=1).fit(features[train], y[train])\n",
    "#     predictions = clf.predict(features[test])\n",
    "#     accuracy = clf.score(features[test], y[test])\n",
    "#     cm = confusion_matrix(y[test], predictions)\n",
    "\n",
    "#     print('In the %i fold, the classification accuracy is %f' %(fold_index, accuracy))\n",
    "#     print('And the confusion matrix is: ')\n",
    "#     print(cm)\n",
    "#     fold_index += 1\n",
    "#############################################################\n",
    "##### Choose by uncommenting on either one\n",
    "#############################################################\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_index = 0\n",
    "for train, test in kfold.split(features):\n",
    "    svm = SVC(kernel = 'linear', C = 50).fit(features[train], y[train])\n",
    "    svm_predictions = svm.predict(features[test])\n",
    "    recall = recall_score(y[test], svm_predictions, average='macro') # \n",
    "    accuracy = svm.score(features[test], y[test])\n",
    "    cm = confusion_matrix(y[test], svm_predictions)\n",
    "\n",
    "    print('In the %i fold, the classification accuracy is %f and the recall is %f' %(fold_index, accuracy, recall))\n",
    "    print('And the confusion matrix is: ')\n",
    "    print(cm)\n",
    "    fold_index += 1\n",
    "################################################################\n",
    "\n",
    "\n",
    "evaluate_model = time.time()\n",
    "print \"--- time to extract features: %s seconds ---\" % (evaluate_model - extracting_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11830,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sitting', 'sitting', 'sitting', ..., 'walking', 'walking',\n",
       "       'walking'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
